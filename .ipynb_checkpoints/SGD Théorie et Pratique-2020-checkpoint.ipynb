{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Descente-de-Gradient-Stochastique\" data-toc-modified-id=\"Descente-de-Gradient-Stochastique-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Descente de Gradient Stochastique</a></span></li><li><span><a href=\"#Mini-batch\" data-toc-modified-id=\"Mini-batch-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Mini-batch</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Descente de Gradient Stochastique\n",
    "===========================\n",
    "\n",
    "La [Descente de Gradient Stochastique](http://en.wikipedia.org/wiki/Stochastic_gradient_descent) est une variante  [stochastique](http://en.wikipedia.org/wiki/Stochastic) de la descente de gradient, qui permet d'utiliser un algorithme efficace pour minimiser les fonctions de coût sous la forme d'une somme:\n",
    "\n",
    "$$\n",
    "  Q(\\mathbf{w}) = \\sum_{i=1}^{d} Q_i(\\mathbf{w}) \\; ,\n",
    "$$\n",
    "\n",
    "où $\\mathbf{w}$ est un vecteur de paramètres (ou poids) à optimiser. La composante $Q_i$ est la contribution du $i$ème échantillon au coût total $Q$, qui est ce que l'on cherche précisémment à minimiser en utilisantun ensemble d'entraînement de $d$ échantillons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En utilisant la descente de gradient standard, $Q$ peut être minimisée par l'itération suivante:\n",
    "\n",
    "$\\begin{eqnarray*}\n",
    "  \\mathbf{w}_{t+1} &=& \\mathbf{w}_t - \\eta \\nabla Q \\\\\n",
    "   &=& \\mathbf{w}_t - \\eta \\sum_{i=1}^{d} \\nabla Q_i(\\mathbf{w}_t) \\; ,\\\\\n",
    "  \\end{eqnarray*}$\n",
    "\n",
    "où $\\eta > 0$ est la taille du *pas*. Cette méthode d'itération par traitement de lots (*batch*) permet de calculer le coût total à chaque pas avec un temps de calcul qui est proportionnel à la taille $d$ de l'ensemble d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "De manière similaire, dans la descente de gradient stochastique, $Q$ est minimisée en utilisant\n",
    "\n",
    "$$\n",
    "  \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla Q_i (\\mathbf{w}_t) \\; ,\n",
    "$$\n",
    "\n",
    "en mettant à jour les poids $\\mathbf{w}$ à chaque itération en utilisant juste un échantillon $i$ choisi aléatoirement de l'ensemble d'entraînement. Ceci est extrêmement efficace pour les grandes tailles d'échantillons, parce que cela permet d'obtenir un temps de calcul de chaque itération indépendant de $d$. Un autre avantage est que cela permet de traiter les échantillons à la volée, comme  une tâche d'[apprentissage en ligne](http://en.wikipedia.org/wiki/Online_machine_learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En pratique, au lieu d'un $\\eta$ fixé, l'algorithme décroît la taille du pas  $\\eta_t$ pour améliorer la convergence.\n",
    "\n",
    "On va implémenter une version avec pas constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dans cette implémentation $Q_i$ a la forme\n",
    "\n",
    "\n",
    "\n",
    "$$Q_i(\\mathbf{w}) = \\left\\Vert \\mathbf{y}_i - f_{\\mathbf{w}} (\\mathbf{x}_i) \\right\\Vert \\; ,$$\n",
    "\n",
    "\n",
    "où $f_{\\mathbf{w}} : \\mathbb{R}^n \\to \\mathbb{R}^m$  est une fonction *modèle* pour notre jeu de données, paramétrisée par $\\mathbf{w}$; $\\mathbf{x}_i \\in \\mathbb{R}^n$ et $\\mathbf{y}_i \\in \\mathbb{R}^m$ sont la paire d'entrées/sorties pour l'$i$ème échantillon de l'ensemble d'apprentissage. Trouver les paramètres $\\mathbf{w}$ qui minimisent $Q(\\mathbf{w}) = \\sum_{i=1}^{d} Q_i (\\mathbf{w})$ et qui ajuste la fonction modèle $f_{\\mathbf{w}}$ à nos données.\n",
    "\n",
    "\n",
    "\n",
    "On va le tester grâce à l'[ajustement de courbe](http://en.wikipedia.org/wiki/Curve_fitting)(*curve fitting*) suivante :\n",
    "\n",
    "\n",
    "\n",
    "$$f_{\\mathbf{w}} (x) = w_1 x^2 + w_2 x + w_3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def f(w,x):\n",
    "    return w[0] * x * x + w[1] * x + w[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Commençons par définir notre jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wt = [0.3,-1.7,2.5] # Ce sont les paramètres que l'on va essayer de retrouver\n",
    "xtrain = np.arange(-10,10,0.01,dtype='f')\n",
    "ytrain = (lambda x : f(wt,x))(xtrain)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(xtrain,ytrain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pour traiter notre ensemble d'entraînement il faut mettre ensemble entrées(*input*) et sorties(*target*), via la fonction `concatenate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.      ,  49.5     ],\n",
       "       [ -9.99    ,  49.423027],\n",
       "       [ -9.98    ,  49.346115],\n",
       "       ...,\n",
       "       [  9.970457,  15.373228],\n",
       "       [  9.980457,  15.416082],\n",
       "       [  9.990458,  15.458996]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrainnp = np.array([xtrain])\n",
    "ytrainnp = np.array([ytrain])\n",
    "dataTconcat = np.concatenate((xtrainnp,ytrainnp))\n",
    "points = np.transpose(dataTconcat)\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On initialise les paramètres $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w0 = torch.zeros(3,requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dans une première version de la *SGD* naïve, on va itérer sur tous les échantillons et incrémenter notre vecteur de poids sur chacun d'eux.\n",
    "\n",
    "On aura une fonction :\n",
    "```python\n",
    "def sgd(f,w,t,eta=0.00001, epsilon=0.001,nepoch=100):\n",
    "```\n",
    "où `eta` est la taille du pas, `epsilon` la limite de convergence du coût, et `nepoch` le nombre maximum de fois où l'on aura parcouru tous les échantillons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercice 1\n",
    "\n",
    "Compléter le code ci-dessous pour faire une descente de gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Installer tqdm et éventuellement ipywidgets\n",
    "# from tqdm import tqdm # Si pas ipywidgets\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "def sgd(f,w,t,eta=0.0001, epsilon=0.01,nepoch=20):\n",
    "    bar = tqdm(total=nepoch) # \n",
    "    epoch = 0\n",
    "    x = t[:,0]\n",
    "    y = t[:,1]\n",
    "    while(epoch < nepoch):\n",
    "        loss = torch.mean(torch.pow(y-f(w,x),2))\n",
    "        loss.backward()\n",
    "        w.data.sub_(eta * w.grad)\n",
    "        w.grad.data.zero_()\n",
    "        bar.update() # Affichage\n",
    "        bar.set_description(\"loss %f\" % loss.item())\n",
    "        epoch += 1\n",
    "        if (loss.item() < epsilon):\n",
    "            print(\"convergence au bout de %i epochs.\" % epoch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175da98a3b344559bfcc18647cc18057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence au bout de 9340 epochs.\n"
     ]
    }
   ],
   "source": [
    "w0 = torch.zeros(3,requires_grad=True)\n",
    "sgd(f,w0,torch.tensor(points),eta=0.0002,epsilon=0.1,nepoch=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On vérifie que l'on s'est rapproché des paramètres du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3, -1.7, 2.5]\n",
      "tensor([ 0.3079, -1.7000,  2.0258], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4743, grad_fn=<NormBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(wt)\n",
    "print(w0)\n",
    "torch.norm(torch.tensor(wt)-w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercice 2\n",
    "\n",
    "Légère simplification, utiliser la fonction de coût toute faite `torch.nn.MSELoss()`\n",
    "```python\n",
    "criterion = torch.nn.MSELoss()\n",
    "```\n",
    "\n",
    "Ecrire la version du sgd avec cette fonction de coût \"prête à l'emploi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sgd(f,w,t,eta=0.0001, epsilon=0.01,nepoch=20):\n",
    "    bar = tqdm(total=nepoch) # \n",
    "    epoch = 0\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    x = t[:,0]\n",
    "    y = t[:,1]\n",
    "    while(epoch < nepoch):\n",
    "        loss = criterion(f(w,x),y)\n",
    "        loss.backward()\n",
    "        w.data.sub_(eta * w.grad)\n",
    "        w.grad.data.zero_()\n",
    "        bar.update() # Affichage\n",
    "        bar.set_description(\"loss %f\" % loss.item())\n",
    "        epoch += 1\n",
    "        if (loss.item() < epsilon):\n",
    "            print(\"convergence au bout de %i epochs.\" % epoch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "w0 = torch.zeros(3,requires_grad=True)\n",
    "sgd(f,w0,torch.tensor(points),eta=0.0002,epsilon=0.1,nepoch=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mini-batch\n",
    "\n",
    "On va maintenant mettre en placer la vraie version *minibatch* du SGD.\n",
    "Pour ce faire, on utilise directement les facilités du [`DataLoader`](https://pytorch.org/docs/stable/data.html)  de `pytorch`, qui combine un jeu de données et un échantilloneur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(points,batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercice 3\n",
    "\n",
    "énumérer tous les éléments de chaque batch avec le print suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"batch %i, échantillon %i : x = %f, y = %f\" % (i,j,x.item(),y.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercice 4\n",
    "\n",
    "Réécrire la descente de gradient version mini-batch en utilisant le `trainloader`.\n",
    "Attention pour le calcul du coût il faut maintenant cumuler sur tous les batch à chaque époque. Bien réfléchir à la bonne stratégie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def sgd(f,w,t,eta=0.0001, epsilon=0.01,nepoch=200,minibatch=10):\n",
    "    epoch = 0    \n",
    "    bar = tqdm(total=nepoch)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    trainloader = torch.utils.data.DataLoader(t,batch_size=minibatch,shuffle=True)\n",
    "    while(epoch < nepoch):\n",
    "        running_loss = 0.\n",
    "        for data in trainloader:\n",
    "            x , y = data[:,0],data[:,1]\n",
    "            loss = criterion(f(w,x),y)\n",
    "            loss.backward()\n",
    "            w.data.sub_(eta * w.grad)\n",
    "            w.grad.data.zero_()\n",
    "            running_loss = running_loss + loss.item() * len(data)\n",
    "        running_loss = running_loss / len(t)\n",
    "        bar.update()\n",
    "        bar.set_description(\"loss %f\" % running_loss)\n",
    "        epoch += 1\n",
    "        if (running_loss < epsilon):\n",
    "            print(\"convergence au bout de %i epochs.\" % epoch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd95ce86157d453dad8286c4b304e4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence au bout de 48 epochs.\n"
     ]
    }
   ],
   "source": [
    "w0 = torch.zeros(3,requires_grad=True)\n",
    "sgd(f,w0,torch.tensor(points),eta=02,epsilon=0.1,nepoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3, -1.7, 2.5]\n",
      "tensor([ 0.3072, -1.7006,  2.0476], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4525, grad_fn=<NormBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(wt)\n",
    "print(w0)\n",
    "torch.norm(torch.tensor(wt)-w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maintenant il nous reste plus qu'à utiliser les optimiseurs de [`torch.optim`](https://pytorch.org/docs/stable/optim.html) comme `Adam`, `RMSProp` etc.\n",
    "\n",
    "il faut charger l'optimiseur avec les paramètres $w$\n",
    "```python\n",
    "optimizer = torch.optim.Adam([w], lr=lr)\n",
    "```\n",
    "\n",
    "Après chaque rétropropagation, on peut appeler `optimizer.step()`: cela mettra effecturea directement le \"pas\" de descente et remet à zéro les gradients, on n'a plus besoin de le faire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercice 5\n",
    "\n",
    "Version du sgd avec l'optimiseur RMSprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def sgd(f,w,t,lr=0.01, epsilon=0.01,nepoch=200,minibatch=10):\n",
    "    epoch = 0\n",
    "    bar = tqdm(total=nepoch)\n",
    "    optimizer = torch.optim.RMSprop([w], lr=lr)\n",
    "    trainloader = torch.utils.data.DataLoader(t,batch_size=minibatch,shuffle=True)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    while (epoch < nepoch):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            x, y = data[:,0],data[:,1]\n",
    "            loss = criterion(f(w,x),y)\n",
    "            running_loss = running_loss + loss.item() * len(data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        running_loss = running_loss / len(t)\n",
    "        bar.update()\n",
    "        bar.set_description(\"loss %f\" % running_loss)\n",
    "        epoch += 1\n",
    "        if (loss.item() < epsilon):\n",
    "            print(\"convergence au bout de %i itérations.\" % epoch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac350a803e79458cb259ff63a9762f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence au bout de 23 itérations.\n"
     ]
    }
   ],
   "source": [
    "w0 = torch.zeros(3,requires_grad=True)\n",
    "sgd(f,w0,torch.tensor(points),epsilon=1e-7,nepoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3, -1.7, 2.5]\n",
      "tensor([ 0.3000, -1.7000,  2.4999], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0001, grad_fn=<NormBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(wt)\n",
    "print(w0)\n",
    "torch.norm(torch.tensor(wt)-w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercice 6\n",
    "\n",
    "Réessayer les sgd avec l'option shuffle=False dans le trainloader. Que constatez-vous ? Quelle explication ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercice 7\n",
    "\n",
    "- Comparer dans la sgd la fonction de coût $L(X,Y)$ sur l'ensemble des points avec la somme des fonctions de coûts des batchs $\\sum{L(X_i,Y_i)}$ déj). Comment expliquer la différence ? Est-ce grave docteur ? (indice, penser à l'inégalité triangulaire)\n",
    "- Même question mais pour la norme du gradient."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": true,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
