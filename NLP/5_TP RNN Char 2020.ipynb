{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Préparation-des-données\" data-toc-modified-id=\"Préparation-des-données-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Préparation des données</a></span></li><li><span><a href=\"#Modèle\" data-toc-modified-id=\"Modèle-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Modèle</a></span></li><li><span><a href=\"#TP\" data-toc-modified-id=\"TP-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TP</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques liens utiles pour la suite \n",
    "\n",
    "[Tutoriel rapide Skorch](https://skorch.readthedocs.io/en/latest/user/tutorials.html) : faire au moins celui du \"Basic Usage\"\n",
    "\n",
    "[Skorch FAQ : How do I shuffle my train batches](https://skorch.readthedocs.io/en/latest/user/FAQ.html#how-do-i-shuffle-my-train-batches)\n",
    "\n",
    "[Skorch FAQ : I want to use sample_weight, how can I do this](https://skorch.readthedocs.io/en/latest/user/FAQ.html#i-want-to-use-sample-weight-how-can-i-do-this)\n",
    "\n",
    "Ce notebook est basé sur le tutoriel pytorch [\"Classifying Names with a Character-Level RNN\"](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = False # Ne mettez à true que si vous pensez avoir un GPU relativement puissant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des données\n",
    "\n",
    "Les cellules suivantes sont extraites du tutoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(unicodeToAscii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compte le nombre de noms par langue, et triez en fonction\n",
    "sorted([(lang,len(category_lines[lang])) for lang in all_categories],key=lambda xy: xy[1],reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneHotEncoding et padding des données\n",
    "\n",
    "(À partir d'ici, ce n'est plus le tutoriel pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la longueur maximale d'un nom\n",
    "MAX_LETTERS= max(reduce(lambda x,y : x+y,[[len(l) for l in category_lines[d]] for d in category_lines]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# \"le\" sert à encoder les langages (\"Arabic\" => 0, \"Chinese\" => 1 etc.)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(all_categories)\n",
    "\n",
    "# letterEncoder sert à one-hot-encoder les lettres \n",
    "# https://fr.wikipedia.org/wiki/Encodage_one-hot\n",
    "letterEncoder = preprocessing.LabelBinarizer()\n",
    "letterEncoder.fit(list(all_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui \"remplit\" les matrices des mots onehotencodés \n",
    "# avec des nan jusqu'à la longueur MAX_LETTERS\n",
    "def lineToPaddedTensor(line):\n",
    "    # transformation en float32 pour pytorch/gpu\n",
    "    lencoded = letterEncoder.transform(np.array(list(line)).reshape(-1,1)).astype(np.float32)\n",
    "    padded = np.pad(lencoded,((0,MAX_LETTERS-len(line)),(0,0)),'constant',constant_values=(np.nan,))\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "letterEncoder.inverse_transform(lineToPaddedTensor('truc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction qui prend en argument un dictionnaire { <language1> : [<liste de noms>], etc.} et \n",
    "# renvoie une entrée X avec les noms one-hot-encodés et la sortie y (label) coresspondant\n",
    "def dictToSamples(d):\n",
    "    tuplesList = reduce(lambda x,y: x + y,[[(y,x) for y in d[x]] for x in d.keys() ])\n",
    "    X = np.array(list(map(lambda xy: lineToPaddedTensor(xy[0]),tuplesList)))\n",
    "    # le type longlong est requis pour les labels dans pytorch\n",
    "    y = le.transform(list(map(lambda xy: xy[1],tuplesList))).astype(np.longlong) \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = dictToSamples(category_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsample = 10000\n",
    "letterEncoder.inverse_transform(X[numsample]),le.inverse_transform([y[numsample]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape # X est un tenseur (nombre de noms, nombre de lettres, taille de l'alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle\n",
    "\n",
    "C'est exactement le même modèle que celui du tutoriel, transformé en modèle standard \"machine learning\" (entrée tensorielle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour compter le nombre de paramètres entraînables dans le modèle\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "F = nn.functional\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward_internal(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, input): # ici l'entrée est \"batchée\"\n",
    "        batch_size = input.size()[0] # taille du batch\n",
    "        max_letters = input.size()[1] # taille du mot\n",
    "        # on initialize le vecteur des sorties cachées\n",
    "        # si on est sur gpu il faut l'initialiser sur le gpu\n",
    "        if USE_CUDA:\n",
    "            hidden = torch.cuda.FloatTensor(batch_size, self.hidden_size).fill_(0)\n",
    "        else:\n",
    "            hidden = torch.zeros(batch_size, self.hidden_size)\n",
    "        # on boucle sur l'ensemble des lettres\n",
    "        for letter in range(max_letters):\n",
    "            # optimisation, si tous les mots du batch n'ont plus de lettres \"réelles\" on s'arrète\n",
    "            # car le reste est du padding\n",
    "            if torch.isnan(input[:,letter]).any(): \n",
    "                break\n",
    "            # et finalement on effectue le forward RNN proprement dit,\n",
    "            # notez qu'on ajoute un axe à la liste des lettres batchées\n",
    "            # pour le \"combined\" (première ligne du forward_internal)\n",
    "            output, hidden = self.forward_internal(input[:,letter].view(batch_size,-1), hidden)\n",
    "        return output\n",
    "        \n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "count_parameters(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier,callbacks\n",
    "import torch\n",
    "\n",
    "# décommenter pour réinitialiser rnn\n",
    "# rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "# Utilitaire pour afficher une barre de progression\n",
    "# pendant l'entraînement\n",
    "callbacksT = [callbacks.ProgressBar(detect_notebook=False)]\n",
    "\n",
    "# Le classifieur skorch\n",
    "net = NeuralNetClassifier(\n",
    "        rnn, # c'est notre modèle\n",
    "        device=('cuda' if USE_CUDA else 'cpu'), #cf. remarque au début\n",
    "        max_epochs=1, # on se limite à un parcours complet de tous les exemples dans l'ordre\n",
    "        batch_size=1, # un échantillon par batch\n",
    "        train_split=None, # on ne fait pas de validation, donc pas de split\n",
    "        lr=0.005, # conseillé pour l'instant\n",
    "        criterion=torch.nn.CrossEntropyLoss, # softmax + nnloss, cas multiclasse simple label\n",
    "        callbacks = callbacksT\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Entraînement\n",
    "net.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction utilitaire qui permet de \"lisser\" une liste de points \n",
    "# (en l'occurence les coûts d'entraînement)\n",
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour l'affichage des courbes d'entraînement (train_loss)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# prend en argument, un classifieur skorch, le nom du score à afficher, par défaut le coût d'entraînement\n",
    "def plot_train_scores(net,score_name='train_loss',smoothing=0.999):\n",
    "    train_scores = reduce(lambda x,y: x+y,net.history[:,'batches',:,score_name])\n",
    "    plt.figure()\n",
    "    plt.plot(smooth_curve(train_scores,smoothing))\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour afficher la matrice de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# prend en argument les entrées, les sorties(classes) et le classifieur\n",
    "def plot_confusion_matrix(X,y,net):\n",
    "    cm = confusion_matrix(y,net.predict(X))\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "    ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "    # Force label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP\n",
    "\n",
    "### A) Premier problème\n",
    "1. Afficher la courbe d'entraînement et la matrice de confusion, que constate-t-on ? D'où vient le problème ? (un indice se trouve dans l'un des liens au début)\n",
    "2. Donner la solution (qui nécessite juste un paramètre supplémentaire dans le classifieur) et justifier.\n",
    " \n",
    "### B) Second problème\n",
    "1. Refaire la courbe d'entraînement et la matrice avec la solution trouvée. Qualifier la courbe d'entraînement par rapport à la précédente.\n",
    "2. Afficher la matrice de confusion : qu'observe-t-on? D'où vient le problème? Comparer avec le code de l'entraînement du tutoriel pytorch original, que fait-il de différent? (indice : nombre de mots par langue) S'aider d'un moteur de recherche pour trouver la bonne dénomination pour qualifier cette démarche. (indice : commence par \"stratified\" en anglais). Justifier.\n",
    "\n",
    "### C) Troisième problème\n",
    "1. Consulter l'aide de la fonction sklearn : ```compute_class_weight```. L'utiliser avec le mode automatique sur notre jeu de données, qu'obtient-on?\n",
    "2. Consulter le lien sur la FAQ skorch concernant l'application des ```sample_weight``` (code fourni dans les cellules ci-dessous). Expliquer ce que fait la nouvelle fonction ```get_loss``` ci-dessous, tout en complétant le code manquant dans les fonctions ```get_loss``` et dans ```unreduced_loss```, qui effectue l'opération inverse de ```get_loss```, mais sur un seul example. (Attention, contrairement au code donné dans la FAQ skorch, on utilise ici les poids sur les classes et non pas sur les échantillons).\n",
    "3. Expliquer pourquoi cette démarche est grosso-modo équivalente à celle du tutoriel original et pourquoi elle résoud le problème précédent.\n",
    "4. Expliquer pourquoi il faut ajouter le paramètre ```criterion__reduction='none'``` au classifieur (consultez la documentation de ```CrossEntropyLoss``` dans pytorch)\n",
    "5. Afficher sur la même graphique les courbes du coût réduit et non-réduit, commenter. \n",
    "6. Contrôler et commenter la matrice de confusion. Comparer avec celle du tutoriel original, quel paramètre simple d'entraînement pourrait-on changer pour obtenir le même niveau de qualité?\n",
    "\n",
    "     \n",
    "### D) Quatrième problème\n",
    "1. Consulter la documentation de ```GridSearchCV``` sur ```scikit-learn```. Executer GridSearchCV avec une recherche sur les paramètres suivants, en prenant bien soin de préciser le scoring :\n",
    "    - le learning rate lr\n",
    "    - la taille du batch \n",
    "    - le nombre de neurones cachés dans le RNN\n",
    "2. Donner le meilleur résultat trouvé par le GridSearchCV, donner le taux de réussite sur le jeu de données et comparer avec le tutoriel original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedNeuralNetClassifier(NeuralNetClassifier):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        self.class_weights = torch.tensor(kwargs.pop('class_weigths').astype(np.float32))\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def get_loss(self, y_pred, y_true, X, *args, **kwargs):\n",
    "        # override get_loss to use the sample_weight from X\n",
    "        loss_unreduced = super().get_loss(y_pred, y_true, X, *args, **kwargs)\n",
    "        loss_reduced = # calculer ici le nouveau coût et le moyenner avec .mean() (on est sur un batch)\n",
    "        return loss_reduced\n",
    "\n",
    "# Cette fonction permet de recalculer le coût original\n",
    "def unreduced_loss(net,X=None, y=None):\n",
    "    loss_reduced = net.history[-1,'batches',-1,'train_loss']\n",
    "    loss_unreduced = # calculer le coût original, ne pas moyenner\n",
    "    return loss_unreduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "callbacksT = [callbacks.ProgressBar(detect_notebook=False),\n",
    "              # On enregistre la fonction de coût original dans l'historique\n",
    "              callbacks.BatchScoring(unreduced_loss,on_train=True,name='real_train_loss')]\n",
    "\n",
    "# Attention il manque ici un paramètre, (celui du premier problème)\n",
    "net = WeightedNeuralNetClassifier(\n",
    "        rnn,\n",
    "        class_weigths = weights_balanced,\n",
    "        device=('cuda' if USE_CUDA else 'cpu'),\n",
    "        max_epochs=1,\n",
    "        batch_size=1,\n",
    "        train_split=None,\n",
    "        lr=0.005,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        callbacks = callbacksT,\n",
    "        criterion__reduction='none'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = smooth_curve(reduce(lambda x,y: x+y,net.history[:,'batches',:,'train_loss']),0.999)\n",
    "real_train_scores = smooth_curve(reduce(lambda x,y: x+y,net.history[:,'batches',:,'real_train_loss']),0.999)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "369.86px",
    "left": "593.2px",
    "right": "20px",
    "top": "136px",
    "width": "668px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
