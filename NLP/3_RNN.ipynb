{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Réseaux-neuronaux-récurrents\" data-toc-modified-id=\"Réseaux-neuronaux-récurrents-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Réseaux neuronaux récurrents</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Réseaux neuronaux récurrents\n",
    "\n",
    "\n",
    "On a déjà vu les les réseaux denses (réseaux de neurones classiques) et les réseaux convolutionnels. \n",
    "=> Ils ont en commun de n'avoir *aucune mémoire*, chaque échantillon est traité de façon indépendante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ce sont des réseaux de neurones dit « à propagation avant » (*feedforward*)\n",
    "Avec ces réseaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On ne peut pas vraiment utiliser des séquences ou des séries temporelles, il manque quelque chose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Traitement séquentiel\n",
    "\n",
    "Quand vous lisez un texte, une phrase, on lit mot par mot tout en ayant en mémoire ce qui précède, ce qui permet d'avoir le *sens* de ce qui est lu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les processus biologiques intelligents traitent l'information incrémentalement tout en conservant un modèle interne de ce qui est traité, construit à partir des informations passées et mis à jour continuellement au fur et à mesure que de nouvelles informations arrivent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN\n",
    "\n",
    "C'est justement l'objet des *RNN*, dans une forme extrêmement simplifiée: on traite des séquences et on maintient un *état* qui contient l'information de ce qui est mis à jour. Un *RNN* a une boucle interne, ce n'est plus du *feedforward*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lorsqu'on a terminé la séquence, on remet l'état à zéro. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Les séquences deviennent les *échantillons* (ou individus) de notre jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La différence avec les *feedforward* est que l'échantillon n'est pas traité en une seule passe :\n",
    "le réseau *boucle* sur les éléments de la séquence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=../_static/img/RNN/RNN.svg width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implémentation naïve en numpy\n",
    "\n",
    "On encode un tenseur 2D (matrice) de dimension `(timesteps,input_features)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- On boucle sur les pas de temps (*timesteps*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- À chaque `timestep` on considére l'état en `t` et l'entrée en `t` (qui de longueur de la séquence, ici `(input_features,)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- On combine l'entrée et l'état pour obtenir la nouvelle sortie à `t` qui devient le nouvel état pour la prochaine itération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On initialise l'état à zéro pour le premier pas de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "state_t = 0 # l'état à t\n",
    "for input_t in input_sequence: # itération sur des séquences d'éléments\n",
    "    output_t = f(input_t, state_t)\n",
    "    state_t = output_t # la sortie précédente devient l'état pour l'itération suivante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Si on veut, on peut exprimer sous forme affine la fonction $f$ et que l'on considère les paramètres $W,U,B$\n",
    "\n",
    "$$f(I_t,S_t) = \\sigma{}(W\\cdot{}I_t + U\\cdot{}S_t + B)$$\n",
    "\n",
    "![](../_static/img/RNN/RNNunrolled.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "state_t = 0\n",
    "for input_t in input_sequence:\n",
    "    output_t = activation(dot(W, input_t) + dot(U, state_t) + b)\n",
    "    state_t = output_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "timesteps = 100 # nombre d'intervalles temporels\n",
    "input_features = 32 # dimension de l'espace d'entrée des caractéristiques\n",
    "output_features = 64 # dimension de l'espace de sortie des caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "inputs = np.random.random((timesteps, input_features)) # entrées aléatoires pour l'example\n",
    "\n",
    "state_t = np.zeros((output_features,)) # on initialise l'état, tout à zéro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W = np.random.random((output_features, input_features))  #\n",
    "U = np.random.random((output_features, output_features)) # création aléatoire de matrices de poids\n",
    "b = np.random.random((output_features,))                 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "successive_outputs = []\n",
    "for input_t in inputs: # vecteurs de forme (input_features,)\n",
    "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b) # combine l'entrée \n",
    "                                                                    # avec l'état actuel (la sortie précédente)\n",
    "                                                                    # pour obtenir la nouvelle sortie\n",
    "\n",
    "    successive_outputs.append(output_t) # stocke cette sortie dans une liste\n",
    "\n",
    "    state_t = output_t # mets à jours l'état du réseau pour le prochain intervalle temporel\n",
    "\n",
    "final_output_sequence = np.concatenate(successive_outputs, axis=0) # le résultat final est un\n",
    "                                                                   # vecteur 2D de forme (timesteps,output_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LTSM (Long Term Short Memory)\n",
    "\n",
    "Ajoutons au réseau récurrent une « porteuse » (*carry*) qui va se rajouter à l'état sur le neurone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![unroll](../_static/img/RNN/LTSM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Attention, la subtilité est que la prochaine valeur de la porteuse est calculée à chaque itération, et que pour ce faire on effectue, non plus une mais 3 transformations. Toutes les trois on la forme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = activation(dot(state_t, U) + dot(input_t, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mais les trois transformations ont leur propres matrices de paramètres! On va les indexer par `i`, `f`, `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "output_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(C_t, Vo) + bo)\n",
    "\n",
    "i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)\n",
    "f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)\n",
    "k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On obtient la prochaine porteuse en combinant les 3 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "c_t+1 = i_t * k_t + c_t * f_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### « Interprétation » du LTSM\n",
    "\n",
    "On peut dire que par la multiplication de `c_t` et `f_t`, on effectue une sorte de filtre sur le flot de données de la porteuse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dans le même temps, `i_t` et `k_t` apportent des informations sur le présent, et mettent à jour cette porteuse avec celles-ci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Le RNN de PyTorch\n",
    "\n",
    "`torch.nn.RNN(*args, **kwargs)`\n",
    "\n",
    "Applies a multi-layer Elman RNN with $tanh$ or $ReLU$ non-linearity to an input sequence.\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$$h_t = \\text{tanh}(w_{ih} x_t + b_{ih} + w_{hh} h_{(t-1)} + b_{hh})$$\n",
    "\n",
    "where $h_t$ is the hidden state at time `t`, $x_t$ is\n",
    "the input at time `t`, and $h_{(t-1)}$ is the hidden state of the\n",
    "previous layer at time `t-1` or the initial hidden state at time `0`.\n",
    "\n",
    "If `nonlinearity` is `'relu'`, then `ReLU` is used instead of `tanh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Args:\n",
    "- input_size: The number of expected features in the input `x`\n",
    "- hidden_size: The number of features in the hidden state `h`\n",
    "- num_layers: Number of recurrent layers. E.g., setting ``num_layers=2`` would mean stacking two RNNs together to form a `stacked RNN`, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1\n",
    "- nonlinearity: The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n",
    "- bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`. Default: ``True``\n",
    "- batch_first: If ``True``, then the input and output tensors are provided as `(batch, seq, feature)`. Default: ``False``\n",
    "- dropout: If non-zero, introduces a `Dropout` layer on the outputs of each RNN layer except the last layer, with dropout probability equal to `dropout`. Default: 0\n",
    "- bidirectional: If ``True``, becomes a bidirectional RNN. Default: ``False``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inputs: \n",
    "    input, h_0\n",
    "- **input** of shape `(seq_len, batch, input_size)`: tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See `torch.nn.utils.rnn.pack_padded_sequence` or `torch.nn.utils.rnn.pack_sequence` for details.\n",
    "- **h_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Outputs: \n",
    "    output, h_n\n",
    "- **output** of shape `(seq_len, batch, num_directions * hidden_size)`: tensor containing the output features (`h_k`) from the last layer of the RNN, for each `k`.  If a `torch.nn.utils.rnn.PackedSequence` has been given as the input, the output will also be a packed sequence.\n",
    "For the unpacked case, the directions can be separated using ``output.view(seq_len, batch, num_directions, hidden_size)``, with forward and backward being direction `0` and `1` respectively. Similarly, the directions can be separated in the packed case.\n",
    "- **h_n** (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for `k = seq_len`.\n",
    "\n",
    "Like *output*, the layers can be separated using ``h_n.view(num_layers, num_directions, batch, hidden_size)``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Attributes:\n",
    "- weight_ih_l[k]: the learnable input-hidden weights of the k-th layer, of shape `(hidden_size * input_size)` for `k = 0`. Otherwise, the shape is `(hidden_size * hidden_size)`\n",
    "- weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer, of shape `(hidden_size * hidden_size)`\n",
    "- bias_ih_l[k]: the learnable input-hidden bias of the k-th layer, of shape `(hidden_size)`\n",
    "- bias_hh_l[k]: the learnable hidden-hidden bias of the k-th layer, of shape `(hidden_size)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All the weights and biases are initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k = \\frac{1}{\\text{hidden\\_size}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0569,  0.3272,  1.1545, -0.5483, -0.8013]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "rnn = nn.RNN(5, 3, 1)\n",
    "input = torch.randn(1, 1, 5)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1285,  0.2534, -0.1705]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0 = torch.randn(1, 1, 3)\n",
    "h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3679,  0.7967,  0.1165]]], grad_fn=<StackBackward>)\n",
      "tensor([[[-0.3679,  0.7967,  0.1165]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, h1 = rnn(input, h0)\n",
    "print(output)\n",
    "print(h1)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
